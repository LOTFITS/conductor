# Servers.
conductor.grpc-server.enabled=false

# Database persistence type.
conductor.db.type=redis_standalone

# Dynomite Cluster details.
# format is host:port:rack separated by semicolon
conductor.redis.hosts=$(Redis-Host):$(Redis-Port):0:$(Redis-Password)

# Dynomite cluster name
conductor.redis.clusterName=$(Redis-ClusterName)

# Namespace for the keys stored in Dynomite/Redis
conductor.redis.workflowNamespacePrefix=conductor

# Namespace prefix for the dyno queues
conductor.redis.queueNamespacePrefix=conductor_queues

# No. of threads allocated to dyno-queues (optional)
#queues.dynomite.threads=20

# By default with dynomite, we want the repairservice enabled
conductor.app.workflowRepairServiceEnabled=true

# Non-quorum port used to connect to local redis.  Used by dyno-queues.
# When using redis directly, set this to the same port as redis server
# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.
conductor.redis.queuesNonQuorumPort=$(Redis-Port)

# Elastic search instance indexing is enabled.
conductor.indexing.enabled=true

# Transport address to elasticsearch
conductor.elasticsearch.url=$(Elasticsearch-Url)

# Name of the elasticsearch cluster
conductor.elasticsearch.indexName=conductor
conductor.elasticsearch.username=$(Elasticsearch-Username)
conductor.elasticsearch.password=$(Elasticsearch-Password)
#conductor.event-queues.amqp.queueType=classic
#conductor.event-queues.amqp.sequentialMsgProcessing=true

# Additional modules for metrics collection exposed via logger (optional)
# conductor.metrics-logger.enabled=true
# conductor.metrics-logger.reportPeriodSeconds=15

# Additional modules for metrics collection exposed to Prometheus (optional)
# conductor.metrics-prometheus.enabled=true
# management.endpoints.web.exposure.include=prometheus

# To enable Workflow/Task Summary Input/Output JSON Serialization, use the following:
# conductor.app.summary-input-output-json-serialization.enabled=true

# Load sample kitchen sink workflow
loadSample=false